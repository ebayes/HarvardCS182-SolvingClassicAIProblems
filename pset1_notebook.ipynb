{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "deepnote_notebook_id": "0443bf03-42a7-4054-83d8-b783d16320b9",
    "deepnote_execution_queue": [],
    "colab": {
      "name": "pset1_notebook.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00000-1ac3d396-210d-4028-b986-1b86815a10fe",
        "output_cleared": false,
        "id": "FZlzwioYjiWP"
      },
      "source": [
        "In this programming assignment, we're going to be implementing an MDP, Value Iteration and Policy Iteration, and at Belief State Updates for a POMDP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "output_cleared": false,
        "cell_id": "00001-f4e120df-7106-4d80-baca-d24ba89185df",
        "id": "3eyWoWUmjiWQ"
      },
      "source": [
        "# MDP formulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "output_cleared": false,
        "cell_id": "00002-75375e15-b744-4ba0-90ff-64a645c1109c",
        "id": "cHVd8JgfjiWQ"
      },
      "source": [
        "The first step is to formulate the MDP using a Transition function, T, and a Reward function, R. Use the templates below and the parameters from problem 2 in the written component to implement T and R."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "output_cleared": false,
        "cell_id": "00003-3e1fd4b2-2281-4946-8e76-741b0b292e3e",
        "id": "c-Qbnn4NjiWR"
      },
      "source": [
        "## Transition function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "output_cleared": false,
        "cell_id": "00004-c16d8669-7ea3-4b0d-b7bd-895652390892",
        "id": "1HbxHblBjiWS"
      },
      "source": [
        "def T(s, a, s_):\n",
        "    \"\"\"Transition function from state s to state s_ via action a.\"\"\"\n",
        "    \n",
        "    if s == 'healthy':\n",
        "        \n",
        "        if a == 'vegetables':\n",
        "            if s_ == 'healthy':\n",
        "                return 1\n",
        "            elif s_ == 'sick':\n",
        "                return 0\n",
        "            elif s_ == 'toothless':\n",
        "                return 0\n",
        "            else:\n",
        "                print('Specify valid state for s_: healthy, sick, or toothless')\n",
        "                return\n",
        "            \n",
        "        elif a == 'candy':\n",
        "            if s_ == 'healthy':\n",
        "                return 0.25\n",
        "            elif s_ == 'sick':\n",
        "                return 0.75\n",
        "            elif s_ == 'toothless':\n",
        "                return 0\n",
        "            else:\n",
        "                print('Specify valid state for s_: healthy, sick, or toothless')\n",
        "                return\n",
        "\n",
        "        else:\n",
        "            print('Specify valid action for a: candy or vegetables')\n",
        "            return\n",
        "    \n",
        "    elif s == 'sick':\n",
        "        \n",
        "        if a == 'vegetables':\n",
        "            if s_ == 'healthy':\n",
        "                return 0.25\n",
        "            elif s_ == 'sick':\n",
        "                return 0.75\n",
        "            elif s_ == 'toothless':\n",
        "                return 0\n",
        "            else:\n",
        "                print('Specify valid state for s_: healthy, sick, or toothless')\n",
        "                return\n",
        "            \n",
        "        elif a == 'candy':\n",
        "            if s_ == 'healthy':\n",
        "                return 0\n",
        "            elif s_ == 'sick':\n",
        "                return 0.875\n",
        "            elif s_ == 'toothless':\n",
        "                return 0.125\n",
        "            else:\n",
        "                print('Specify valid state for s_: healthy, sick, or toothless')\n",
        "                return\n",
        "        else:\n",
        "            print('Specify valid action for a: candy or vegetables')\n",
        "            return\n",
        "\n",
        "    elif s == 'toothless':\n",
        "        if s_ != 'toothless':\n",
        "            return 0\n",
        "        elif s_ == 'toothless':\n",
        "            return 1\n",
        "    \n",
        "    else:\n",
        "        print('Specify valid state for s: healthy, sick, or toothless.')\n",
        "        return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "output_cleared": false,
        "cell_id": "00005-a0c2acde-bc75-4293-b097-d0a1e1cf29d6",
        "id": "sI-qpIqjjiWV"
      },
      "source": [
        "## Reward function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00008-ace52f8b-04ea-4631-9de3-77e053c5468f",
        "output_cleared": false,
        "id": "lk8cb4KvjiWW"
      },
      "source": [
        "def R(s, a, s_):\n",
        "    \n",
        "    if s == 'toothless':\n",
        "        return 0\n",
        "    \n",
        "    if a == 'vegetables':\n",
        "        return 4\n",
        "            \n",
        "    elif a == 'candy':\n",
        "        return 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "output_cleared": false,
        "cell_id": "00007-d284ae18-ef37-4086-8b83-4134b9566cf3",
        "id": "Ltt4WKYTjiWY"
      },
      "source": [
        "# MDP Solvers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "output_cleared": false,
        "cell_id": "00008-a85dfd5f-790b-4411-b973-13136085ad0a",
        "id": "E3DRO-8ejiWZ"
      },
      "source": [
        "Now that you've implemented T and R, implement Value Iteration and Policy Iteration using the templates below and the pseudocode presented in class or in the book."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "output_cleared": false,
        "cell_id": "00009-ca961531-a05c-4d76-b1a5-0f604f6120bf",
        "id": "1UceXE6mjiWZ"
      },
      "source": [
        "## Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "output_cleared": false,
        "cell_id": "00010-4acb175d-37ba-473d-b549-edd386bf8d0f",
        "id": "sp37dePKjiWa"
      },
      "source": [
        "S = [\"healthy\", \"sick\", \"toothless\"]\n",
        "A = [\"candy\", \"vegetables\"]\n",
        "epsilon = 0.0001\n",
        "gamma = 1\n",
        "\n",
        "def valueIteration(S, A, T, R, gamma, epsilon):\n",
        "\n",
        "    # initialize dict for Q \n",
        "    Q = {}\n",
        "    for s in S:\n",
        "        for a in A:\n",
        "            Q[s+\"-\"+a] = 0\n",
        "\n",
        "    while True:\n",
        "        # create temp dict Qnew\n",
        "        Qnew = Q.copy()\n",
        "\n",
        "        # initialise delta to 0\n",
        "        delta = 0\n",
        "        \n",
        "        # loop over state space \n",
        "        for s in S:\n",
        "            for a in A:\n",
        "                # update optimal policies using bellman equation\n",
        "                Qnew[s+\"-\"+a] = sum(T(s, a, \"-\") * (R(s, a, \"-\") + gamma * Q[s+\"-\"+a]))\n",
        "                # update delta \n",
        "                delta = max(delta, abs(Qnew[s] - Q[s]))\n",
        "                # store in Q\n",
        "                Q = Qnew.copy()\n",
        "        print(Q)\n",
        "        if delta < epsilon * (1 - gamma) / gamma:\n",
        "            return Q   \n",
        "\n",
        "    pi = Q.copy() \n",
        "    return pi \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "output_cleared": false,
        "cell_id": "00011-8f5bfd78-306b-4551-ade1-c00f189f85cc",
        "id": "ABiklWebjiWc"
      },
      "source": [
        "## Policy Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00013-3ef9d1cb-ee65-439c-b0f3-14d89a40941c",
        "output_cleared": false,
        "id": "cu0p18wKjiWc"
      },
      "source": [
        "S = [\"healthy\", \"sick\", \"toothless\"]\n",
        "A = [\"candy\", \"vegetables\"]\n",
        "epsilon = 0.0001\n",
        "gamma = 0.9\n",
        "\n",
        "def policyIteration(S, A, T, R, gamma, epsilon):\n",
        "\n",
        "    # initialise dict for V and pi\n",
        "    V = {}\n",
        "    pi = {}\n",
        "    for s in S:\n",
        "        V[s] = 0\n",
        "        pi[s] = A[0]\n",
        "    \n",
        "    while True:\n",
        "\n",
        "        # policy evaluation\n",
        "        delta = 0\n",
        "\n",
        "        for s in S:\n",
        "            Vprime[s] = V[s]\n",
        "            # evaluate using fixed policy using bellman equation\n",
        "            V[s] = R(s, pi[s], \"\") + gamma * sum((T(s, pi(s), \"\") * Vprime[s]))\n",
        "            delta = max(delta, abs(V[s] - Vprime[s]))\n",
        "       \n",
        "        if delta < epsilon * (1 - gamma) / gamma:\n",
        "            return V\n",
        "       \n",
        "        unchanged = True\n",
        "        \n",
        "        U = V.copy\n",
        "\n",
        "        # extract better policy based on these new values\n",
        "        for s in S:\n",
        "            for a in A:\n",
        "                Vprime[s] = U[s]\n",
        "                pi[s] = R(s, a[s], \"\") + gamma * sum((T(s, a[s], \"\") * Vprime[s]))\n",
        "                delta = max(delta, abs(pi[s] - Vprime[s]))\n",
        "        if delta < epsilon * (1 - gamma) / gamma:\n",
        "            return pi\n",
        "        \n",
        "        for s in S:\n",
        "            if V[s] != pi[s]:\n",
        "                pi[s] = V[s]\n",
        "                unchanged = False\n",
        "        if unchanged:\n",
        "            return pi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "output_cleared": false,
        "cell_id": "00013-3eb23e1a-e5ba-469d-b406-c70ebbf445f3",
        "id": "rxMsmhG2jiWe"
      },
      "source": [
        "## Finding the $\\gamma$ threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "output_cleared": false,
        "cell_id": "00014-8ea779fe-0b57-4b61-8313-80cf32bbeac1",
        "id": "mLaxy4I2jiWf"
      },
      "source": [
        "You may have noticed that the optimal policy is typically to eat candy in every state. Find the smallest, up to 0.0001 error, discount factor $\\gamma$ such that for $\\pi$, the optimal policy, $\\pi$(sick) = vegetables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "output_cleared": false,
        "cell_id": "00015-00f8864c-4516-45b2-a599-ea8d8e41b333",
        "id": "AEkVdX_ajiWf"
      },
      "source": [
        "def find_gamma(valueIteration, epsilon=0.0001):\n",
        "    \n",
        "    # equated calculations from question 2 b of the written part to solve for Vpi2(s) - Vpi1(s) = epsilon, then rearranged in terms of gamma and computed below\n",
        "\n",
        "    Sick_Candy = valueIteration('sick', 'candy', \"\", \"\", \"\", epsilon)\n",
        "    Sick_Vegetables = valueIteration('sick', 'vegetables', \"\", \"\", \"\", epsilon)\n",
        "    Healthy_Vegetables = valueIteration('healthy', 'vegetables', \"\", \"\", \"\", epsilon)\n",
        "    gamma_min = (24 + 8 * epsilon) / (2 * Healthy_Vegetables + 6 * Sick_Vegetables + 7 * Sick_Candy)\n",
        "    return gamma_min"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "output_cleared": false,
        "cell_id": "00016-92e28392-2f18-45bf-9c19-dd7c2c650783",
        "id": "Dl79SzcbjiWi"
      },
      "source": [
        "# POMDP Belief States"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "output_cleared": false,
        "cell_id": "00017-47f5b529-46fd-4c93-a825-f3185c170540",
        "id": "pP_rd0f_jiWi"
      },
      "source": [
        "In problem 3 of the written you computed belief state updates by hand. To do this you had to compute the evidence posterior $\\mathbb P(e | s)$ and run the FORWARD updates $$b(s') = \\alpha \\mathbb P(e|s') \\sum_{s \\in S} \\mathbb P(s' | s, a) b(s).$$\n",
        "Implement the evidence posterior, O, below and the update_belief_state method using the FORWARD update. You can use this to check your answer for problem 3(c) in the written."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "output_cleared": false,
        "cell_id": "00018-a574fdc7-45e4-4e95-89d8-1d483f3611cd",
        "id": "QmASjyw4jiWj"
      },
      "source": [
        "## Evidence posterior"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "output_cleared": false,
        "cell_id": "00019-f3e68525-d513-487e-8979-da798ea07fac",
        "id": "48lUK_K_jiWj"
      },
      "source": [
        "def O(evidence, state):\n",
        "\n",
        "    if evidence == 'F':\n",
        "        if state == 'toothless':\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "            pass\n",
        "    else:\n",
        "        temperature = evidence\n",
        "        # State Posteriors\n",
        "        P_healthy_given_T_minus = 0.9\n",
        "        P_sick_given_T_minus = 0.1\n",
        "        \n",
        "        # Evidence Posteriors\n",
        "        P_healthy_given_T_plus = 0.01 \n",
        "        P_sick_given_T_plus = 0.99 \n",
        "        \n",
        "        # Evidence Priors\n",
        "        P_T_minus = (84/89) \n",
        "        P_T_plus = (5/89) \n",
        "        \n",
        "        # State Priors\n",
        "        P_healthy = 0.85 \n",
        "        P_sick = 0.15 \n",
        "\n",
        "        if temperature == 0:\n",
        "            \n",
        "            P_temperature = P_T_minus\n",
        "            \n",
        "            if state == 'healthy':\n",
        "                P_state_given_temperature = P_healthy_given_T_minus\n",
        "                P_state = P_healthy\n",
        "            elif state == 'sick':\n",
        "                P_state_given_temperature = P_sick_given_T_minus\n",
        "                P_state = P_sick\n",
        "            elif state == 'toothless':\n",
        "                P_state_given_temperature = 0\n",
        "                P_state = 0\n",
        "                pass\n",
        "            else:\n",
        "                print('Specify healthy or sick for state!')\n",
        "                return\n",
        "            \n",
        "        elif temperature == 1:\n",
        "\n",
        "            P_temperature = P_T_plus\n",
        "            \n",
        "            if state == 'healthy':\n",
        "                P_state_given_temperature = P_healthy_given_T_plus\n",
        "                P_state = P_healthy\n",
        "            elif state == 'sick':\n",
        "                P_state_given_temperature = P_sick_given_T_plus\n",
        "                P_state = P_sick\n",
        "            elif state == 'toothless':\n",
        "                P_state_given_temperature = 0\n",
        "                P_state = 0\n",
        "            else:\n",
        "                print('Specify healthy or sick for state!')\n",
        "                return\n",
        "        else:\n",
        "            print('Specify binary variable for temperature!')\n",
        "            return\n",
        "        \n",
        "        if evidence != 'F':\n",
        "            if state != 'toothless':\n",
        "                P_temperature_given_state = (P_state_given_temperature * P_temperature)/(P_state)\n",
        "                return P_temperature_given_state \n",
        "            else:\n",
        "                return 0\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "output_cleared": false,
        "cell_id": "00020-2f4a68c2-51ba-4e92-a6d2-fc2c8ecac937",
        "id": "MPqQdjkrjiWl"
      },
      "source": [
        "## Belief state update "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW55av_G23zB"
      },
      "source": [
        "epsilon=0.0001\n",
        "\n",
        "\n",
        "def update_belief_state(temperature, action, belief_state):\n",
        "\n",
        "    # initialize belief state tuple --> b(s)\n",
        "    BS = belief_state\n",
        "\n",
        "    # initialise evidence given new belief state --> P(e|s')\n",
        "    if temperature == 0:\n",
        "        P_evidence_given_s  = O(0, 'sick')\n",
        "        P_evidence_given_h  = O(0, 'healthy')\n",
        "    elif temperature == 1:\n",
        "        P_evidence_given_s  = O(1, 'sick')\n",
        "        P_evidence_given_h  = O(1, 'healthy')\n",
        "    elif temperature == 'F':\n",
        "        P_evidence_given_t = 1\n",
        "\n",
        "    # initialise new belief state given belief state and action --> P(s'|s,a)\n",
        "    if action == 'candy':\n",
        "        P_s_given_h = T('sick', 'candy', 'healthy')\n",
        "        P_s_given_s = T('sick', 'candy', 'sick')\n",
        "        P_h_given_h = T('healthy', 'candy', 'healthy')\n",
        "        P_h_given_s = T('healthy', 'candy', 'sick')\n",
        "    if action == 'vegetables':\n",
        "        P_s_given_h = T('sick', 'vegetables', 'healthy')\n",
        "        P_s_given_s = T('sick', 'vegetables', 'sick')\n",
        "        P_h_given_h = T('healthy', 'vegetables', 'healthy')\n",
        "        P_h_given_s = T('healthy', 'vegetables', 'sick') \n",
        "\n",
        "    A = P_evidence_given_h * (P_h_given_h * BS[0] + P_s_given_h * BS[1])\n",
        "    B = P_evidence_given_s * (P_h_given_s * BS[0] + P_s_given_s * BS[1])\n",
        "    b_0 = A/(A+B)\n",
        "    b_1 = B/(A+B)\n",
        "    U = (b_0, b_1, 0)\n",
        "    \n",
        "    updated_belief_state = U\n",
        "    return updated_belief_state "
      ],
      "execution_count": 22,
      "outputs": []
    }
  ]
}